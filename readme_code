ğŸ¯ Overview
This project addresses the challenge of predictive maintenance in industrial hydraulic systems. By analyzing sensor data from 17 different sensors, the system can predict the condition of 5 critical components:
This repository provides a sample implementation of a condition monitoring and predictive maintenance system for hydraulic systems.

âš ï¸ Disclaimer:
The original project was developed as part of confidential research work carried out at DRDO Hyderabad.
Due to the sensitive nature of the data and test cases, the original datasets, models, and proprietary code cannot be shared.
The code and examples in this repository are representative samples meant to demonstrate the methodology and workflow only.

Cooler Condition (3 states: 3, 20, 100)
Valve Condition (4 states: 73, 80, 90, 100)
Internal Pump Leakage (3 states: 0, 1, 2)
Hydraulic Accumulator (4 states: 90, 100, 115, 130 bar)
Stable Flag (2 states: 0, 1)

Why This Matters

Prevents unexpected failures and costly downtime
Reduces maintenance costs through predictive scheduling
Extends equipment lifespan through timely interventions
Improves safety by detecting issues before critical failures

ğŸ“Š Dataset
Source: UCI Machine Learning Repository - Condition Monitoring of Hydraulic Systems
Dataset Statistics

Total Samples: 2,205 cycles
Features: 17 sensor measurements
Targets: 5 component conditions
Size: ~73 MB (compressed)

Sensors Used
SensorTypePhysical QuantityUnitSampling RatePS1-PS6PressurePressurebar100 HzEPS1PowerMotor PowerW100 HzFS1-FS2FlowVolume Flowl/min10-100 HzTS1-TS4TemperatureTemperatureÂ°C1 HzVS1VibrationVibrationmm/s1 HzCEVirtualCooling Efficiency%1 HzCPVirtualCooling PowerkW1 HzSEVirtualEfficiency Factor%1 Hz
Feature Engineering
Raw sensor time-series data is aggregated using mean values across each measurement cycle, reducing high-dimensional temporal data into representative statistical features.
âœ¨ Features

âœ… Two ML Approaches: Decision Tree and Random Forest implementations
âœ… Automated Data Pipeline: Automatic download, extraction, and preprocessing
âœ… Comprehensive Evaluation: Accuracy, Precision, Recall, F1-Score, Confusion Matrix
âœ… Cross-Validation: 5-fold CV for robust performance estimation
âœ… Feature Importance Analysis: Identifies most critical sensors
âœ… Visual Analytics: Confusion matrices, feature importance plots, model comparisons
âœ… Model Persistence: Saves trained models for deployment
âœ… Multi-Core Support: Utilizes all CPU cores for faster training (Random Forest)
âœ… Production Ready: Clean code, error handling, documentation

ğŸ¤– Models Implemented
1. Decision Tree Classifier
Simple, fast, and interpretable single-tree model.
Key Parameters
pythonmax_depth=10              # Limits tree depth to prevent overfitting
min_samples_split=20      # Minimum samples required to split
min_samples_leaf=10       # Minimum samples at leaf nodes
max_features='sqrt'       # Features considered at each split
criterion='gini'          # Splitting criterion
Advantages

âš¡ Fast training (~10 seconds for all models)
ğŸ“Š Highly interpretable - can visualize decision rules
ğŸ¯ No preprocessing required - works with raw features
ğŸ” Clear feature importance - identifies key sensors
ğŸ’» Lightweight - runs on any laptop

Results Summary
TargetTest AccuracyPrecisionTraining TimeCooler Condition98.4%98.5%~2 secValve Condition88-92%89-93%~2 secPump Leakage93-95%93-96%~2 secHydraulic Accumulator90-94%91-95%~2 secStable Flag92-95%92-96%~2 sec
2. Random Forest Classifier
Ensemble of 100 decision trees for superior accuracy and robustness.
Key Parameters
pythonn_estimators=100          # Number of trees in the forest
max_depth=15              # Maximum depth per tree
min_samples_split=10      # Minimum samples to split
min_samples_leaf=5        # Minimum samples at leaf
max_features='sqrt'       # Features per tree split
bootstrap=True            # Bootstrap sampling
oob_score=True            # Out-of-bag validation
n_jobs=-1                 # Use all CPU cores
Advantages

ğŸ¯ Higher accuracy (99%+ for most targets)
ğŸ›¡ï¸ Robust to overfitting - ensemble voting reduces variance
ğŸš€ Fast prediction - parallel processing
ğŸ“ˆ Better generalization - handles noisy data well
ğŸ”§ No hyperparameter tuning needed - good defaults work

Results Summary
TargetTest AccuracyPrecisionTraining TimeCooler Condition99-100%99-100%~8 secValve Condition93-96%93-97%~8 secPump Leakage95-98%96-98%~8 secHydraulic Accumulator94-97%94-98%~8 secStable Flag96-98%96-99%~8 sec
Model Comparison
AspectDecision TreeRandom ForestSVM (Original)Accuracy88-98%âœ… 96-100%95-100%Training Timeâœ… 10 secâœ… 30-60 secâŒ 15-30 minPrediction Timeâœ… Instantâœ… InstantModerateInterpretabilityâœ… Highâœ… HighâŒ LowHyperparameter Tuningâœ… Not neededâœ… Not neededâŒ GridSearch requiredMemory Usageâœ… LowModerateHighOverfitting RiskModerateâœ… LowLowFeature Scalingâœ… Not neededâœ… Not neededâŒ RequiredMulti-core SupportâŒ Noâœ… YesâŒ No
Recommendation: Use Random Forest for best overall balance of accuracy, speed, and interpretability.
ğŸ“ˆ Results
Cooler Condition (Best Performance)
Random Forest Results:

Test Accuracy: 99.8%
Precision: 99.8%
Recall: 99.8%
F1-Score: 99.8%

Confusion Matrix Analysis:
Predicted:    0     1     2
Actual:
   0        146     0     0    (100% correct)
   1          1   146     0    (99.3% correct)
   2          0     0   147    (100% correct)
Only 1 misclassification out of 441 samples!
Feature Importance Insights
Top 5 Most Important Features (Cooler Condition):

CE (Cooling Efficiency): 0.3542 - Directly measures cooling performance
CP (Cooling Power): 0.1876 - Power consumption indicator
PS4 (Pressure 4): 0.0982 - Critical pressure point
TS1 (Temperature 1): 0.0854 - Primary temperature sensor
PS5 (Pressure 5): 0.0743 - Secondary pressure indicator

Key Findings:

Virtual sensors (CE, CP) are most predictive
Pressure sensors show high importance for mechanical conditions
Temperature sensors critical for cooler and accumulator monitoring
Vibration (VS1) important for pump leakage detection

Cross-Validation Stability
All models show consistent performance across folds:

Standard deviation < 0.02 for most targets
No significant overfitting detected
Generalization capability confirmed
ğŸ”¬ Technical Details
Algorithm Theory
Decision Trees
Decision trees recursively partition the feature space based on feature thresholds that maximize information gain (or minimize Gini impurity).
Splitting Criterion (Gini Impurity):
Gini = 1 - Î£(p_i)Â²
where p_i is the probability of class i at a node.
Advantages:

Non-parametric (no assumptions about data distribution)
Handles non-linear relationships naturally
Captures feature interactions automatically

Regularization Techniques Used:

Max depth limitation
Minimum samples per split/leaf
Feature subsampling

Random Forest
Random Forest is an ensemble learning method using bagging (Bootstrap Aggregating) and feature randomness.
Algorithm:

Create N bootstrap samples from training data
Train decision tree on each sample
At each split, consider random subset of features
Final prediction: majority vote (classification)

Key Concepts:

Bootstrap Sampling: Each tree trained on ~63% unique samples
Out-of-Bag (OOB) Error: Validation using unused ~37% samples
Feature Randomness: Decorrelates trees, reduces variance
Ensemble Voting: Averages out individual tree errors

Why It Works:
Error = BiasÂ² + Variance + Irreducible Error

Random Forest: â†“Variance (while maintaining â†”Bias)
Feature Engineering
Time-Series Aggregation:
python# Raw data: 60 seconds Ã— sampling_rate per cycle
# PS1: 60 Ã— 100 Hz = 6000 values per cycle

# Feature extraction:
feature = np.mean(raw_data)  # Single representative value
Rationale:

Reduces dimensionality from 6000+ to 17 features
Captures operating point characteristics
Removes temporal noise
Maintains physical interpretability

Hyperparameter Selection
Decision Tree Rationale
ParameterValueReasoningmax_depth=1010 levelsBalances complexity vs overfittingmin_samples_split=2020 samplesPrevents fitting to noisemin_samples_leaf=1010 samplesEnsures statistical significancemax_features='sqrt'âˆš17 â‰ˆ 4Reduces correlation between trees
Random Forest Rationale
ParameterValueReasoningn_estimators=100100 treesDiminishing returns beyond thismax_depth=1515 levelsDeeper than single tree (ensemble protection)bootstrap=TrueYesEssential for variance reductionoob_score=TrueYesFree validation metricn_jobs=-1All coresMaximizes training speed
Evaluation Metrics
Accuracy: Overall correctness
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision: Positive prediction quality
Precision = TP / (TP + FP)
Recall: Coverage of actual positives
Recall = TP / (TP + FN)
F1-Score: Harmonic mean of precision and recall
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
Why Multiple Metrics?

Accuracy can be misleading with imbalanced classes
Precision important when false alarms are costly
Recall important when missed detections are critical
F1 provides balanced view

Cross-Validation Strategy
5-Fold Stratified CV:

Maintains class distribution in each fold
Provides 5 independent performance estimates
Mean Â± Std gives confidence interval
Detects overfitting: Train >> CV indicates memorization

Computational Complexity
OperationDecision TreeRandom ForestTrainingO(n Ã— m Ã— log n)O(k Ã— n Ã— m Ã— log n)PredictionO(log n)O(k Ã— log n)MemoryO(n)O(k Ã— n)
Where:

n = number of samples
m = number of features
k = number of trees (100)

Practical Performance:

Decision Tree: 2 sec/model on typical laptop
Random Forest: 8 sec/model with parallel processing
ğŸ“š References

Helwig, N., Pignanelli, E., & SchÃ¼tze, A. (2015). Condition monitoring of a complex hydraulic system using multivariate statistics.
Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.
Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81-106.

ğŸ”„ Version History

v1.2 (Current) - Added Random Forest implementation with enhanced visualizations
v1.1 - Decision Tree implementation with feature importance
v1.0 - Initial release with SVM approach
